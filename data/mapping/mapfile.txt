SLIDE1
00:00:00 all right good evening welcome back from spring break a fun-filled events I assume I hope you enjoyed the last piece set of something that I was having a lot of fun with myself over for my Christmas break when I was first player on that stuff
00:00:20 and I thought it would be an appropriate way to play around with with Luigi so I hope that you enjoyed that and are able to finish that tonight without much further the difficulty we are going to continue into desk and Park
00:00:40 a little bit before the the midterm and we're really going to get started on it now I think it's a fascinating topic that really kind of helps evolve how you do big data science at least within python
00:01:00 Elsa helps understand somebody high-level objectives that we're talking about I'm in terms of storage mechanism just getting into the aspect of it as well as the the daggs in understanding your work as a graphical workflow
00:01:20 one thing that I wanted to point out is that I think there's been a few misses in terms of communication so Piazza is where we convey as a group on and student feedback between each other is Great Sands
00:01:40 is that are visible to everyone I think it's working pretty well for that but we haven't been super successful in Direct Communications on on Piazza Messina number post where people try to contact you know a specific TA or they ask for grading feedback and because there's so much stuff on Piazza
SLIDE2
00:02:00 in a formal way it's easy for us to to lose that and it's nothing personal we're not trying to snub anyone is it's just we're not all necessarily on Piazza in a dedicated fashion Andrew treated more like a group Collective where you know we get approximately everything
00:02:20 good at handling dreka news with communication so yes that you do that type of communication on do Canvas right so if you need to make an appointment for office hours you to send a direct message to another person that you're trying to contact on canvas if you have a question about your your grade on a particular thing use the the
00:02:40 I meant to discuss that our if you have overall questions on on grading either use the direct messages on canvas to do that that's something that'll no sense to look at your emails and we can you know reply more specifically
00:03:00 there's a little bit of distributed responsibility sometimes that happens there because none of us are quite sure who's been conversing with an individual student and who's really on point not for that that communication so please try to be direct on canvas we need that type of thing and let's just try to keep Piazza
00:03:20 visible posts for group discussion on I think that will it will help ensure that the communication standards remain I released the next step for the final project so hopefully you got some good inspiration
00:03:40 what are the bonus lectures that that we got out to you so that was Rihanna's work the week before the midterm and then we had Aaron and Joanna present their projects on the midterm election if you were taking the midterm and didn't get a chance to to see that Bunch material
SLIDE3
00:04:00 on campus as well should take a look at it in to be posted the slides at as well right so that should give you a couple ideas of example projects for your final independent think it'd be nice to have a proposal in the deadline on April Fools
00:04:20 make sure they have something that you've thought through right so we'll be a graded proposal but you don't have to have everything nailed down perfectly and if you change your mind about something that's fine we just want to make sure that the process is moving and you guys are thinking through this so the proposal scope is pretty Broad
SLIDE4
00:04:40 do your own individual learning objectives so there's a lot of things that can satisfy the requirements are we saw a number of a successful project last semester where you know the deliverable with a pull request in open source software and that's that's great right there's nothing better like doing something real that benefits the community
00:05:00 white sofa if there's a package with some bugs or some features that you think would be great to contribute to make those pull request give us a little bit of a write-up about how it relates to this class and how you sort of structure and that would be a fantastic project before we have no
00:05:20 hearing you so if there is a problem at work that you think is related to this course you can use your work time to solve that give us a little bit extra and terms of another right up and like the design and a formality of it but you can use your time at work to solve a problem for this class as well as long as it's it's related to material
00:05:40 if you write a problem set that's also a fantastic idea right to take it off if you're interested in especially if we didn't really cover it that well or in-depth in a problem set write it up as a problem set give us a solution as well so we don't have to figure it out on the road and
00:06:00 if it's good enough we could even use it next semester or I'm all about tools right if there is a tool that that you think would be useful for this course whether it's as a student or as an instructor or just as an interesting thing that's it
00:06:20 person send me an idea already which I thought was pretty funny and a neat so this person wants to download the lecture videos and run like a voice to text on this lecture video and do like some you know classification and topic extraction like
00:06:40 on the lecture so that like when you're studying you can like search like the topics by keyword I'm sitting pretty cool I don't know whether not like the Technologies are there for for this person to pull that off but anything that like houses do our work even if that work is this class you know his is great
00:07:00 so let's get those proposals in on write a proposal can just be like a document you know a couple of paragraphs of what you're thinking so give you you know the week in the weekend to try to come up with something here it should be well thought out right so a proposal that's like
00:07:20 usually the Ouija to explore problem is not you know particularly deep and improved thawed out so think about the problem think about what hasn't been invented that you need to invent for it whether it's like a particularly juicy on the issue between you know say integrating desk for Luigi or something
00:07:40 it's fine if you want to work as a team we just need to be able to tie it back to individual grades so in your proposals you can say I want to work with someone so I'm going to do this part they're going to do this part you could figure that part out later when we go to great at the end of the day we need individual grades
00:08:00 somewhere along like 10 to 20 hours so you know you guys may be rolling your eyes because I say the problems that should be 5 hours and you're obviously not spending that so unexpected this is some you know multiple of of a problem set but hopefully it's just your work that you find interesting and engaging
00:08:20 I'm as I said the proposals are non-binding but we will ask after you've had your sort of proposal approved if you want to make a significant deviation from it I'm you should run that idea by you know someone on the teachings. Just make sure you're not know off the in the woods the most successful projects
00:08:40 science course this is not this is this is a python course this is not a data science course so we had a couple of projects last semester of that were you know I'm going to analyze some data or I'm going to run some you no sentiment analysis or I'm going to run you know this cool classifier those are cool projects
SLIDE5
00:09:00 class right we need to focus on advancing our python skills and you can go do all the awesome cool data science course working in data science courses should you be taking the focus needs to be python in and coding there are a couple of non python things you could do so late
SLIDE6
00:09:20 I'm going to start talking about visualization which includes like JavaScript and HTML and CSS as well as ikpi so there's a couple of sort of none python but related things that you could potentially focus on instead of purely python but it has to be very tightly within the python
00:09:40 we just fight all right I'll talk about Dak so before and I think I've seen a few things casually but at the high-level desk is going to give us a couple of neat things are going to change the way that we think
00:10:00 some of the work that we do first and foremost that you'll notice when you start playing with it it is a symbolic tool set that means that you see an object which isn't actually the object that you care about its some symbol about how to compute that right so it's
SLIDE7
00:10:20 that's not you know that's not to insult it's it's it's an object that is lazily evaluated using the term delayed but in the back that allows us to compute
00:10:40 blade results rice or going to be seeing dags all over again and ask is going to provide us an API with manipulating that computational graph whether or not we realize that we're doing that directly a lot of the time you're not working with the desk API itself you working with high-level rappers around it which is which is great
00:11:00 also has executors so you can actually run it's on a cluster we're not going to do that in this course but it's important understand because probably will do that at some point in your work you can use Dash is fine when your laptop as well or on your desktop but you can actually distribute the competition on a cluster
00:11:20 distributed workflow ask is a set of rappers around things like numpy pandas and scikit-learn now where you can use objects that look a heck of a lot like those types of things but aren't there their symbolic and they just kind of copy the interface
00:11:40 so that's kind of the framework going to go into on but it will change the way that we think about it when we look at it like this it kind of gives us the feeling that it's like a free lunch that we can just use and have distributed computation and that's kind of true
00:12:00 will start changing the way we think about data and walked in on some of that tonight in this course and if we think about it in the right way we can actually do things faster in the significant way so it will see how that
00:12:20 this is the lowest level of desk and you're probably not going to actually use this all that much but I think it's worth going through briefly to understand so at the heart of the desk is this delayed function delayed is kind of a function is often used as a decorator so you can have like a function like
SLIDE8
00:12:40 can be a delayed function just by using his decorator in this case I think I'm just going to return you know whatever happens to be plus one I can also use it directly so if I wanted to have a delay data is called delayed on say you know operator ad and now I have a function that is delayed so it delayed function when you
00:13:00 don't actually evaluate the function it will return a symbol that says I later want to evaluate it with the arguments that were passed in so ink of 1x is no longer to it is a symbol that says call Inc
00:13:20 some point later when I tell it to same thing with Y and here because I know I told you that these were delayed right I can call add-on other delayed objects as well right and so Z is the symbol that doesn't just say call add-on X&Y but it says actually go compute X and Y
00:13:40 I need to actually come get them in order to compete myself right so everyday object is going to have a compute method and that's what actually gives us the end result we can double-check this here so and could one is two and come to us three 2 + 3 is 5
00:14:00 a visualize function which is kind of nice you can actually see the the dag that's used to to calculate that that object that's going to look something like like this this isn't exactly is that it gives us but it's it's close enough
00:14:20 get a couple of other things so the object has a key so this is the the sort of node that represents the final computation of of Z and it has this interface desk graph which I'm just going to turn into a basic dictionary here to look at it at
00:14:40 so the key is the the key in the scratch that contains the result right so they look that up the value in here is this sort of special Temple which says call Operator. Add using the results from these other keys
00:15:00 what does up and this is the result of calling ink on on one and ink on on to write and we can see that as a graph as well so we have these values one into their being passed to the increment function the results of that is this key
00:15:20 value associated with this keep that is in pasta the odd function in our final value for Z can be accessed using this node so this is all they're like this is the the back end of desk which you rarely ever use
00:15:40 a lot of things on top of this and sometimes if you need to build up your own functions in desk you do need to modify the graph directly and it's it's worth it to have sort of a basis for what's going on or is is
00:16:00 I've never leaves the numpy variance themselves although I'm sure some data scientist do but pandas are Workhorse for a lot of what we do in data science so it makes sense that we can use to ask for that so he's going to give us chunked versions of
SLIDE9
00:16:20 we call this out of memory because we can operate on every trunk without having enough memory on computer tax restore everything in Ram at once it also is going to work for text files or probably I can talk about that too much but the basic Primitives and desks are arrays dataframes in bag
00:16:40 collections of text story or Jason that are chunky up in similar ways it's going to junk these out in one dimension for dataframe is but a raise can be multiple Dimensions whatever dimensionality better than Empire I can take you can have trunks and every direct you can distribute calculation for each jump across the cluster and
00:17:00 call delayed objects as well and at least for me I find that it is flexible and powerful enough that I don't really use spark anymore that's a bit of hearsay for data scientists these days but we can scale things up to the same level as Park does if we can deploy ask on a cluster and we can stay Within
00:17:20 the python ecosystem while doing that what people ask me why don't I like spark you know spark is pretty easy to use it's not really about that for anything worse Park is easy to use such as a simple Group by or what not like that's fine that's when you know you start getting complicated functions they want to map out onto
00:17:40 dataset and you want to spend your time developing those functions in Python because that's our native language we can do better testing or more comfortable with it we might want to be using numpy Primitives instead of learning the equivalence in Scala or or Java in Sebastopol I was to not have to do any of that junk if we don't like feel comfortable in
00:18:00 so I really go to spark anymore these days at all in order to understand that we have to understand how a problem can be broken down so let's take a classic data science problem
00:18:20 buy some column and summing it a lot of our problems look like this and they can be reassembled and stitched back together based on their pieces right so we can imagine if we could partition out this dataframe we can get the same results by grouping each of those partitions
SLIDE10
00:18:40 come together and because of the properties of addition adding those results back up will give us the same result right not every function is going to have this property but if what we want to do can be broken down into a chunk like this is going to be our friend
00:19:00 think of Princess. Products in the same way right so we have some Matrix which is the dot product between matrices A and B right we can kind of think of that element was right the results for any row and column in Matrix m is this you know some
00:19:20 play in that comment see and then taking the inner product so long that you have the matching Dimensions we can also think of that like in block form right so we might have our Roblox in a and C column blocks and beat
SLIDE11
00:19:40 windows can be represented as like the corresponding. Products are added together across the intervention of a blocks there right so we can break it down and then kind of just concatenate those results into matrix-m as a block
00:20:00 right so that might look something like this this is not real code that you should ever use but just to help us think through the problem right so here Matrix and Matrix B I'm calling them block matrices because they're not the real matrices who care about this is going to have some shape I'm in
00:20:20 in this Matrix is actually a matrix itself right if we can think of chunking out our Matrix like that in this case I happen to say you know it's it's a delayed Matrix so we can have rows according to a columns according to be in this block dimension
SLIDE12
00:20:40 numpy.. Has delayed function and we can think of just iterating through every outer row and every other call him zipping through the Align dinner blocks and then adding those results
00:21:00 right for some block Holloman sunblock route and at the heart of it that's what is delayed. Product between two daska raise or two. Matrices is going to do it right we can always sort of dropped down into this type of logic if we want to do you know
00:21:20 of linear algebra or even just you know tensor product or whatever happens whatever happens to be the functions that that's the second time I've done that
00:21:40 I need to be banned from from drinking liquids there's almost like there should be a sign right here so happens to be
SLIDE13
00:22:00 is Eddie Brock a this is water this time
00:22:20 we don't want to do this one use the built-in functionality. Dash provides us in a way that looks like numpy but under the hood if we say like that's great. Another that's great it's going to be doing something
SLIDE14
00:22:40 the underlying principle is this notion of split apply combine so in the Big Data world this is also often called like mapreduce which is kind of a similar thing in the sort of python in an r and in small data science in pandas we see the terminology split up
SLIDE15
00:23:00 right you can think of it like this or basic Group by the dataframe is going to be split by the group in column thank you each split is now the particular
SLIDE16
00:23:20 care about in this case you know we're selling it so that's an application on each group and then the combined is the concatenation of it now this works just fine for for small data right where we can sort of shuffle all of the values
00:23:40 Cantonese different groups on the 22 out of memory at a time with a little bit differently this is still going to happen but at a lower level so
00:24:00 zoom out a little bit and because we're dealing with Partition dataframes in participant raise that split has already occurred right so the split in desk is the partition that you're dealing with and the reply is that same problem that we just saw but
SLIDE17
00:24:20 so you'll take the first chunk of your your dataframe call group I. Some and you're probably going to have you no more than one you know group within that chunk unless you happen to be in a partitioned already in a specific way so the
00:24:40 different right it's not just concatenate but it's actually coming in this case the results from every participant with an implicit you know if there was like no Groupon one is going to fill it with zero rights to war base
00:25:00 enjoying the unique set of groups that we came up with so the combined in the apply look a little bit different but it's still the same Paradigm as long as we can calculate the correct results within each part without caring about the values in the other parts
00:25:20 then we can we can go to this this process can't do that which which are important right the nice thing about desk is that it it is sort of solved most of the data science problems for us already so in general you can just use it exact
00:25:40 the other dataframe were right you can take a function of a dataframe and you know for instance add two columns together and if this is a pandas dataframe it will just give you a new pandas dataframe if it's destined for him it will give you a new desk dataframe with a computational graph that you know
SLIDE18
00:26:00 mission for you right so things will work seamlessly most of the time element wise operations all all things like that we're going to work just fine and what's the other API is just going to be copied directly right so if we let you know we can read something from parque instead of reading it via pandas we ask
00:26:20 play some function the same way we could apply that in pandas or group by and mean and Josh is going to handle the you know appropriate split apply can buy at that higher-level calling pandas. Group I'd. Some or whatever within each partition in the combining the results in the approach
00:26:40 right so this is the large part will just work despite the fact these are different classes there's a few different things though so map partitions that partitions is kind of like apply but it calls me function on every constituent dataframe itself right
00:27:00 gets a dataframe which happens to be one of those partitions as opposed to apply where the function is going to get a row exactly the same way that the pandas does it right to map partitions is like new to desk it's not something that pandas has a concept of notice however that
00:27:20 meta it we started talking about this the last time we talked about desk is it we can start dealing with things a little bit differently in order to preserve the schema right so here's where things start looking a little bit different so
00:27:40 Sweden pandas dataframe from Excel to turn it into desk if we're not reading a directly with typically have to get out in some way right so this is going to say hey I wanted to frame where every partition is only 10 rows and when we converted from pandas
SLIDE19
00:28:00 well because it has all the structure there so it has the metadata Sonos what columns it hasn't knows the data types of those columns and knows how many participants we have and also knows the index divisions for those partitions that's something that's really really important
00:28:20 similar to the hosta defiled you guys got this is from last semester so there was no just as a 10 x 5 petition so there's 50 students in this data set and none of them are you guys we can then do things again somewhere.
00:28:40 look up student 57019 809 right so we no longer have five positions with the data we only have one partition and the minimum and maximum value for the index is the student that we looked up cuz we know
00:29:00 meditate about this the state of rain that there could only be the student in here there are no students with higher or lower on IDs because we looked it up on the index when talk about this more on today
00:29:20 this is something called a predicate pushdown kind of and it's dropping it down to a single partition also noticed using the number of nodes and the graph right so you want from 5 to 6 cuz we only had to do one operation on one of those partitions so you can keep track of how come
00:29:40 scrap is getting a couple of things that I want to point out that I've seen people get confused about and think that they're doing themselves a favor by trying to take a shortcut and it kind of makes it harder to understand and used ask appropriately
00:30:00 thing that I see people doing when they start learning desk is to call compute all the time they want to let having trouble getting you know their desk function to work so they call computer and then they process it and pandas and then they turn it back into desk and that defeats the purpose
SLIDE20
00:30:20 you want everything to be lazy until you actually need the result typically you don't actually need the result typically you want to save the result weather's to parque or something else right so if you find yourself saying hey compute doing something in pandas turning it back to desk and then writing it out
00:30:40 it probably means you've sort of made a mistake in your understanding and there's some reason why you couldn't get it to work an in-dash can you want to try to avoid that we can't do everything in desk there are operations that need to know what day it is in order to do them
00:31:00 is transpose so a dataframe knows its columns but not its Rose so if you were to transpose it it would turn the rows into columns right so there's no way to not know the rose and turn that in
00:31:20 columns using a symbolic delayed dataframe write such a simple simple case there's plenty of other things like even a unigroup by right if you Group by then you'll lose the partitions assume you lose the
00:31:40 you know divisions cuz there's enough to know what values are in a column in so there's there's a few operations where if you do need a desk objects you need to compute it in some form because it can't actually figure out what the graph should be like
00:32:00 running the data there's nothing for things like that where we know we need to call compute we have to tell it that but you implied earlier that if you're going to park a example that that would
00:32:20 implicitly is that the other cases were complicit in mother's where we where it isn't or is it always one of the other computer for you directly is that a yes
00:32:40 storing it someplace and there's like there's a few other small exceptions but the most part it's only when you're riding at the desk will it directly compute it there's a few other cases on so one of them is dataframe. Head or it returns first five it
00:33:00 do that and so it just does that for you on but for the vast majority of times it will be there maybe a couple of places where it might compute some small sample of things
00:33:20 most party won't one one area where I'm searching my memory which might be weird so when you create a
00:33:40 so when you apply function like this I try to always give it the metadata and part of the reason for that is because it sometimes needs to know that
SLIDE21
00:34:00 and sometimes the way that it finds it out if you don't Supply it is it will run part of them through the grass and then infer it right so it's a little bit annoying to bring this back to the car static typing days where it's like hey you know this function could return a floating-point it could return a dataframe with six
SLIDE22
00:34:20 you don't know because it's not a statically typed language so if that's needs to know what that schema is you either have to tell it and be correct about it or it has to run some of the day that through and hope that that day does representative so there's a couple of places where it will do some small computations like that which are kind of
00:34:40 cuz there's no guarantee that they're right anyway right imagine like if that first partition is like empty right so I usually try to be explicit with like the metadata I can't think of any other functions off the top of my head that actually run
00:35:00 without telling you they tend to just not Implement them in the desk library but I'll see if I can think of any more examples like that
00:35:20 funktion-one acting like a stupid late it has to be delayed getting to tell a function functions Delight yes you you don't have to decorate things as delayed typically yourself right so so map
SLIDE23
00:35:40 will automatically take this function and make it delayed for you for instance the what you would do to those are methods or anything that's in itself if
00:36:00 you have a function that you're writing yourself you can decorate it is delayed for a number of reasons that when you call it with a non delayed object or even with delayed obvious it'll it'll work right you can't just call a function using delayed object if its function
SLIDE24
00:36:20 because of my time I felt so typically when you're writing your own functions and using them directly as opposed to going through the task API that's when you need to decorate things as delayed
00:36:40 a dealer mistake that I see a lot is repositioning so people tend to get pretty quickly that having divisions is good in a dataframe and sometimes we'll take a look when they're dividing and say hey my dear friend doesn't have visions anymore let's set them
SLIDE25
00:37:00 call repartition or something like that typically means that there's a bug in your workflow write a reproduction is going to take a while cuz you have to reshuffle the day so it's much better to actually go back and figure out where you've lost the partitions you don't keep partitions all the time because there are some
SLIDE26
00:37:20 you will inherently lose the divisions such as a group by right but if you are doing a sort of vanilla extract and then you won't be changing what your index is you should you should be keeping
00:37:40 but understanding what the divisions are and how big your petitions are is sort of the best first thing you can do in order to make sure that your daskas is behaving appropriately right to making sure that your divisions are where they want that your petitions aren't too small right you don't want to have three rows per partition you want to have
00:38:00 the hundreds of megabytes worth of data for a given partition that sound like a big data that you can be smaller for Fordyce right but you want let's say it like a gigabyte of data you might want you know 10250 partition something like like that
00:38:20 take significant time to compute because the overhead for actually dealing with partition is on the order like milliseconds something like that right so you don't want to be spending more time in the overhead of dealing with the partition then you want to be actually calculating whatever you want to calculate on that petition right a little bit
00:38:40 so understand the divisions and try not to force anything this until you really know what you're doing cuz it's usually masking a flaw in your in your Justin Moore flower coat so with that said we can talk a little bit more about
00:39:00 that you're operating in and how that that data is is going to work so I think I introduced has already going to do a little bit of a deeper dive so when we're dealing with structured data there's two basic ways of storing at on
SLIDE27
00:39:20 and column stores so a lot of us are used to working with cs fees so if we take for instance another Titanic is a toy dataset that's pretty common machine learning right and look something like this passenger Eddie was another survived morbid dataset and their name their names are pretty funny
SLIDE28
00:39:40 a hundred years old in super duper British but we look at that and we think it's two-dimensional and easy to look up and that's because we're humans right and it's printed on the screen but that's not the way the data actually looks right this is what it looks like and they're these special characters that separate the rose
00:40:00 columns commas separating of the column this is called a row storage and schematically it looks like this on on desk right any CSV is you're going to have no record number one call him ABC in the number to ABC
00:40:20 specific record or specific column more than the whole thing we just have to read it like in a whole bunch of chunks and jump around a lot if these things are fixed with then it's helpful sometimes but if any single one of them is like a character array or string by we have no principles way of
00:40:40 within the state of store to get anything that we want we've introduced the notion of a column store and are certain gold standard for that is is parquet and they were talking about more and more of in 4k is going to collate the data by column so it won't look like a 1B 1C one but it's going to be all of the values
00:41:00 followed by all of the values Crombie Falabella concert from seeing that has a number of advantages one it's really easy if you only want to read one of those columns from disc enough to read the entire thing you can just scan it it also if you for any column that is
00:41:20 miniature floating points you get Random Access within that column in so it's very easy to you know to read a bunch of today we're talking about things that have for the most part they fixed set of columns
00:41:40 change over time but most were talking about things that we can think of a tabular data and the rules will change when we talked about unstructured stuff pretty amenable to sharting right so you can imagine that a data set is a collection of Park files one of those Park files or CSV
00:42:00 I'm in they can have different rows stored in the file right or different ranges of rose right so we can think of it like this and you might call that a vertical Shard or we might also call that like a row group there's number of words for that and we can think that you
SLIDE29
00:42:20 Lowe's 1 through 3 are stored in this particular part file there's one of the fundamental data store that I want to talk about a little bit today because it's going to change the way that we think about some of these operations and that's the notion of a key-value store
00:42:40 as good as something like parque but it gives us a different set of of strength so usually key value store is often used when you're dealing with you know an unfixed schema but that's
SLIDE30
00:43:00 today we can think of a key value store is is basically anything that's going to associate a single key with one or more values in this case the value is actually you know composite
00:43:20 on disk is going to give you fast look up and and scans on the key value but nothing else you know dictionary is is a key-value store in memory right but we have to buy stores I can be written to disk in and offered on effectively as well right so dumb
00:43:40 but the point of this is that you get very fast look up and search on the key Dimension which is going to be sorted or possibly even random access over lucky or partitioned along the key but everything else is going to be stored inside this Valley
00:44:00 from the point of view of a distributed system such as desk we're going to come back to that we're going to dive into parquet in a little bit more or detail so anything about parque is that it's going to store statistic
00:44:20 each column and row group in the file itself so typically that includes like the number of unique values obviously didn't type but more importantly for numeric and some string column types in a store the minimum and maximum value Within
SLIDE31
00:44:40 I'm as a bit of metadata that you can read directly it will also talk about like which row groups and how many exist in in the number of parts and that's that's that's useful to that date is actually stored in two places
00:45:00 it's actually stored in every part file so if you're looking at a data set with multiple part files you can actually just read a single one of them and you get all the statistics about the row groups in that part file but optionally partaken store these there there's actually two slightly different ones and I forget
00:45:20 but sometimes see a metadata file and it common metadata file I believe it's this one which is important which actually stores all of the column statistics for all of the row groups the advantage here is
00:45:40 show me like desk is going to read both of these just fine but if you're reading something often it's really really nice to have it all in one file parquet and like spark were like initially designed for hdfs
00:46:00 icon Prime clusters where it's relatively easy to just scan through into a thousand Park files and they look at the header on them worthy can you can read that these days we're dealing with things like S3 and remote distributed systems and if you have to read a bit of every single file
00:46:20 I'm it starts getting slow in and unwieldy you have to list all of the files in S3 and they have to download a bit of each one of them and it becomes unwieldy so it's really really nice to ensure that your data comes in one single place that's cool right a park
00:46:40 by default and so if you're writing. In the reading back it's great spark however and does not so if you are dealing with a workflow that is using spark in 4k this is a really nice tipped you can turn this on cuz I think since like version two it disabled by default you can
00:47:00 and you can be much much nicer to Downstream dusk client when you go back to it to read that it soaked little bit of a pro tip be there so why are these statistics interesting we want the statistics because we're going to build up a predicate pushdown
00:47:20 1 most important thing you can do for performance when you're dealing with with large data pushdown refers to when we're scanning and looking up data this is the equivalent of a where clause in SQL we want to push that filter as far into the detention as possible
SLIDE32
00:47:40 don't want to do is load up everything into pandas and then scan through every row to see where the condition recruiting for his trip what we want to do is Leverage The parquet file statistics determine if we need to read that file in the first place to get the day that we want and if we successfully do that we will say that we have pushed down our query logic
00:48:00 or the predicate right this is where things like pandas actually can swimming. Can actually make things faster right we're normally talking about. As a way of scaling or working symbolically but we can actually get dramatic performance increases
00:48:20 if we Leverage The metadata appropriately in and that's something that something like pandas by default isn't going to be doing right so we can actually get huge performance gains not just stealing things out but natural gain here many of you have probably done the same thing whether you know it or not when you're dealing with like a traditional date
00:48:40 if you're clearing for values in a database it's much faster if there's an index on that on that thing in a distributed system we don't have indices typically so what we do is we're going to come up with with the system that gives us a predicate pushdown and that's the
00:49:00 in a distributed data scenario so what is this look like here's a pretty common query in SQL form just for for readability but you can do the same thing and vanilla. Right we want to select a certain
00:49:20 did you know where a different column or possibly the same problem doesn't matter has a value that's between some range right the direct way to utilize a predicate pushdown in desc
SLIDE33
00:49:40 the top-level re-park a function right so when you're first reading it in from parque weiner selecting column be called a projection pushdown I think in the predicate pushdown happens via specifying these filters so I can say
00:50:00 unable to whatever my value for a 2 is and is less than or equal to whatever that value for a 3 years and this gives us a huge amount of specificity for what we read right we say hey we're only looking at this one and only looking at these Rose wear this
00:50:20 and so you know the fraction of data that we read in order to do that is very very small compared to the fraction to do that when she want to get back right we can ideally approach you know a sort of false positive ratio of 0 in terms of rows that we read
00:50:40 use right we can get pretty specific so a lot of these things are ours are detailed in these sort of columnar on datastore reading said I've assigned you so very very powerful right we can accelerate or workflow dramatically by not reading in the data that we don't care about
00:51:00 and it's not like a top-level citizen in the python ecosystem so one thing that you don't realize until the third time you do this is that this is not actually filtering the data what this is doing is filtering the row groups so it will not return a Chung
SLIDE34
00:51:20 where those Rose can't have values of interest but it's not actually going to filter the rose in a chunk that might have a hit right so you still have to actually run the query so you'll gets awkward code that looks a little bit like this right we have
SLIDE35
00:51:40 we still have to say hey still go into the pandas API and actually filter down do you know the specific value that you care about right a little bit awkward and it's an optimization technique but it's not a query itself so we have to be aware of that
SLIDE36
00:52:00 the current best default parque engine is not the one that asked works best with because of this tire always what we used initially which is the official pandas parquet think it seems to implement everything in
SLIDE37
00:52:20 except for predicate pushdowns which is a huge huge oversight there's a very long thread it's not an oversight they know about this I just don't know why or when is going to get into the code you can read this thread where you know West McKinney comments on the difficulty
00:52:40 the is the party Library designed by the desk people for desk and so it starts by implementing predicate pushdown and it does that quite well so you find yourself in this scenario that when you're using pandas you probably one of the faults of 2 pi r
00:53:00 even though you suggested friends look like a lot of pandas you want to be using fast 4K and when you need to do a piece set on a desk which you know hopefully we'll really soon you want to make sure you're using that otherwise you're not going to be able to do any any pushdowns that that you might need to do it right
00:53:20 you know I've I've looked pretty extensively through some of the you know the Fast Park a and in the desk code myself and it does it does the job but they've got a couple of like code styles that are little bit fragile and so I've noticed like flat-out bugs
00:53:40 General it's it's pretty hard to hold a stick to the parque in the pandas Community with a code is pretty excellent and these guys had a great job too but it's it's not necessarily as good of a quality as I've seen some pretty shaky code
00:54:00 edible bugs in it and there's been like that Alicia's and what not and you have to be aware of that in done and kind of Juggle. That trade-off as part of your Advanced python work
00:54:20 to have in this Parts this one's this one's long so let's take a five-minute break first and then I'll come back I'll come back to this all right
SLIDE38
00:54:40 restarts about some the final project so if you're looking for ideas take a look at the slides that we released for last semester that will give you an overview of of where else we're going through the rest of the course so that includes Jango and and
00:55:00 on that one but dealing with you know live databases and sort of web interfaces and templates those kinds of things world going to deal with apis swagger is a cool tool that that we explored a little bit as a way of of automatically generating
00:55:20 a python package that will let you interact with either your own server or some other server if you define the API definition cython and numbers way of optimizing code when you need that that tight Loop and some visualization stuff as well so that's like bouquet
00:55:40 she was the what's the other thing in pipe is blinking on displaying large data
00:56:00 is mustard give you a little bit more video of where we're going and how that might fit into to project and have one of the partitioning aspect of desk because we just talked about how to use the
00:56:20 and now we need to know a little more about how to design the partitions in order to best take advantage of the predicate pushdown and all the other ways that we can leverage you know metadata in order to optimize our our big data tools so
00:56:40 two ways that you can do pretty good push down informally and in dusk so 1 is what we would just talk about via Park itself and the other is via the index we've mentioned this already but now we can juxtapose the the to use
00:57:00 here's the brake push down and here is the index and poops and if we can leverage our query to utilizing index it will be not exactly the same as as a
00:57:20 so the divisions are the minimums of the index for every partition and so in this particular case like partition zero you know it starts with student g733 and the
00:57:40 text Vegas to should be in - 19877 and then there's this sort of special number that they put the end which in my opinion is actually a design flaw that the last partition to the last division is the maximum value
SLIDE39
00:58:00 in the last partition I call that a design flaw because it means you can't actually get a partition out of the desk thing and then concatenate them back together you actually lose information when you do that it seems a bit silly and I've got a couple of complaints and pull request that you can read
00:58:20 until I are considered to be a design while but it is what it is and we can still leveraged index quite effectively since we know that this relationship exists in the divisions give us information about what's stored on on the index in there for the rose so if we simply
00:58:40 a value of an index I mentioned before that it's only going to look at the single partition at which and knows that that could be the case right and we can verify that by visualizing the task craft right so here we have the initial task graph for all of the five partitions
SLIDE40
00:59:00 was recalled Lok notice it's only reading into partition one right now we see our six tasks one two three four five six but if we were to go to evaluate them it actually wouldn't if you want to be with this it actually wouldn't bother to read these at all right we
00:59:20 I can push down in this case because we know that we only ever needed this one petition after all of our computation is is done on that graph that's great if we can do that that's awesome and great job of basically designing every single part of its package to you know you
00:59:40 if it can write not every operation can use the index but if it can you can dress that desk is probably considered the best way to do that and that's in the the API that does mean of course you have to make sure that divisions are set in your bed frames and that you don't lose them by doing certain operations or sometimes if there's an order of operation
01:00:00 do the one that loses the index last and that way the other ones can be that you know can use in Texas as far as long as I Got Stripes but we find ourselves realizing a sort of fundamental
01:00:20 insurmountable design obstacle is not Park a gospel Reed parque and load of a desk object but that's where the relationship stops right itself is never going to use 4K metadata and any of these for the partition magic or filtering or anything
SLIDE41
01:00:40 or have to use it as an index or you have to directly call something on Reed parque or you're out of luck that you can't use everything that parquet gives you at any random point in. Sparkles a better job of this hands down
01:01:00 two sticks whenever it can probably because it's directly to me and sometimes that will be meaningful
01:01:20 more like pandas then it's like sparkle parque and we're not going to ever fully bridge that Gap but we can do other things that kind of give a similar power if you can't match it perfectly so one of the things that I like to do with desk is too real
01:01:40 as an indexed data frame with divisions steel frame is really a key-value store right it happens to be tabular rights is not a generic key-value store that can restore everything but the index is the key and the
SLIDE42
01:02:00 The Columns that are associated with that and we can treat it as an optimal key-value store if we keep the divisions and we designed the index based on what we want so we have to think about how to do that
01:02:20 a generic pandas dataframe this is a key-value store and if we use the index appropriately we can gain power from it so stick example here so here's your students learning
01:02:40 let's add some columns to this so we've had this for two semesters now so here's all the students that were you know from 2018 fall and if we add you guys into the state of frame we'd have your new valleys in here the 2019 spring something like that in pandas
SLIDE43
01:03:00 the luxury of being able to use a multi-column Index right we can say set index of these two columns in order and it'll look something like this right and if you saw more of them you would see like all the people for the first semester laps and then it would go to ALDI and then it would sort
01:03:20 in each of those groups support multi-column index he's but there's nothing that stops us from creating a surrogate index from that and we're going to get into the habit of designing an index that gives us the Quarry power that we want so if I
01:03:40 single semester at a time I would want to put that semester as the first part of my index because I want a query based on a prefix typically right prefixes matter first when your sorting so we want whatever we going to Korea by first to be the first part of our key
01:04:00 and the second part is going to be the student ID in this case right but it could be whatever we want we're coming up with a single value that is a composite of all the values that we want to to query we might you know calculator that like this right
01:04:20 and we are going to then set that as the index and we probably have to restart it right in and restore it but once we read it back it'll look something like this fall 2018 then with the student use their notice how we have a lot of tasks now
01:04:40 indexing the thing to begin with is going to be expensive task because it can't use the predictions directly it's going to have to reshuffle things over designing this to be searchable the way that we want to search it so
01:05:00 now it happens it's easy to get all students for a single semester right I just have to know the bounds so the first potential key for a given semester is you know just that semester with the empty Dash in the last
SLIDE44
01:05:20 encoded in that particular case so you can never have you know something higher than S right so I just know how they designed and I can do it like that right so I can do a location range that queries all of the keys that could ever potentially have that semester but I have to know the relation
01:05:40 value that I care about right that's be designed into the system if I deviate from that that like a query design then I run into a little bit of trouble right so I can find a single student if I know what semester that student was in
01:06:00 your house that is and I can say hey I know you're in Spring 2019 and so I can plug it into my career plan and I can calculate the exact can I can see if you're here but if I don't know what semester you took the class and just know you're getting a buddy and therefore your hash I have to do one of the few things I could
01:06:20 I could come up with a list of all the semesters that are possible and then search for all of those that's what I did here but there's no guarantee that I know that right for something that's time-bound right over it
01:06:40 but if you are on if your first call him that you're using in this the surrogate key is a value that you don't necessarily know all the potential values of you have to calculate that by scanning your your data on coming over or something
01:07:00 look up of this you can't simply scan on that column anymore right so we've created work for ourselves but if we do it appropriately we can leverage the structure of this key value store in an Optima white or lease is up
01:07:20 we throw some more complexity at you and turns out we've been using the term partition and partition is a poor choice of words.
01:07:40 as literally just a like a part file right and we can think of that as like a flat partitioning scheme so a single partition here will show up as a single partition and. Dataframe in that form typically
SLIDE45
01:08:00 looks something like this if you're used to like Hive or or something like that inside your data set you will have a folder which has all of the Rose for a given value of a column petitioned up as a subgroup right this is really what a partition means so we would have all the same
01:08:20 or at least a subset of those part files but only four rows in which in this case semester to semester equals fall 2018 and then I might have another folder follicles spring 2019 or whatever happens to me and it will have a new set of part files right that's what I
01:08:40 when you're talking about like hdfs for hire for or spark or something like that anything but desk and we'll get a little bit confused between that because we'll tend to use them interchangeably because that's does so hopefully we can we can clarify that so you can when you write something out to park a you can
01:09:00 use a parquet partition scheme like such right this is now not the desk partition scheme this is the bar Cape Regency and that'll work great except that so
01:09:20 we read it back in this case right and it'll work just fine where it doesn't work is if you participate in something that is not shorted along the key long the index so
SLIDE46
01:09:40 let's say that we have another column such as are you a grad student or not now the way we written out this key obviously it has no bearing well technically everyone in the course is grad student wasn't true or the semester
SLIDE47
01:10:00 work because I'm already sorting it on this Composite Index that's the key right I'm petitioning on semester but that's already accounted for it in the column in in the in the index
01:10:20 this other thing when I read it back notice how I go from 5 - partitions to 9 and I've lost my divisions that's because desk is creating these
SLIDE48
01:10:40 write the radical choose only got it was false and the initial partitions in the dataset then get filtered to only the ones where you know that has a particular value written back out but now we have direction should have been 10 but one of having to be empty so now we have those 10-part files or 9
SLIDE49
01:11:00 and they come back in the order that you see them on disk write radicals one comes first alphabetically in 02 for Ford radicals is false and we'll see the keys in the
01:11:20 index and uses Park files go when it tries to stick them in order they're no longer like monotonically increasing that's thinks it's an unsorted index when it sees that and it just throws it away it has no way of knowing that
01:11:40 internally in each one of those and that true or false so unless them and like load each one separately and merge back together it's just
01:12:00 this this division so it's pretty frustrating all the sudden can't actually partition things if you want to keep using it inside the desk or quote you can actually go back in and say hey don't give me everything only give me things for grad equals true and then we'll come back with the visions and then do the other one
01:12:20 but gospel lose the actual partitioning scheme if you write it out and read it back not interested person can lose lose your index and it's quite frustrating
01:12:40 few other I think these are ones that I wrote as well so you can see some extra extra things if you do the stars explicit predicate pushdown where you then go back and read like just where you know that, true or false or an index
SLIDE50
01:13:00 that's just up the level of the reaper cake it's not at the desk interface so you got to be careful when you start using the more advanced features here or go to spark it's not that difficult to
01:13:20 code that allows a single part file at a time as a single part. Dataframe and then kind of operates them all on as a collection that's how it's going to work around it in the past I didn't get to like production quality for that color was just an experiment that I was doing with right but you can kind of forget about
01:13:40 can I have multiple partitions and just pretend laws one and then operate that as a collection and in that kind of that kind of works but I've never gotten that to a pull request that I wanted to submit
01:14:00 actually use this in my my actual work so the next couple of slides are out of scope for anything that we're actually in a test you on a little scary so don't want to cause too much work but I think it's a good example of the power of this technique
SLIDE51
01:14:20 to help you guys flush out know your understanding so this is this is for my work on this this project but you can actually see it total homeschool.com the notion here is that Liberty Mutual my employer we know a lot about about people and in Risk
SLIDE52
01:14:40 new GPS devices that we give our policyholders to put in the car and we can measure whether somebody is a safe driver in that context there marginalizing roadsafe the safety of of driving per person we wanted to look at it differently can we figure out what roads are safe and maybe we can monetize at Cigna
01:15:00 do you know like a real estate aggregator something like that where we can say hey on average people driving over this road are safer and there for me if you got young kids that's a good signal for you to live in that neighborhood something like that
01:15:20 scale and partly because you need to do a lot of neighborhood convolution so for every point on a map you need to look at all of the nearby points and that's a very very hard problem you know computationally so I'm going to walk you through kind of how I approach that using using desk and using this
01:15:40 indifference so here's the heart of the problem I showed you how you can do like predicate pushdowns and those kinds of things on a key value store and that's relatively easy when you're sorting a one-dimensional key write anything that turns out to be
01:16:00 text can be sorted right but it's the world is not one dimension right the world is too kind of emotions and you can't sort something in two dimensions
01:16:20 can say is sort first by latitude and then my longitude but that yields particularly poor partitioning in particular poor performance right if I want to search for All Points within a region I don't want to have to scan all the latitudes
01:16:40 within the region of latitude and then search through every single one of those for the reason of longitude I guess, like the least efficient query that I can do so the question is can we turn into National signal or three dimension if you count time into a one-dimensional index that we can leverage using. And that's what I
01:17:00 the trick here is to use something called a space-filling curve so this is a picture of what's called a z order crook it's called the order because it has a z on the side here and it's not the best one of these curves but it's the easiest one to talk through
01:17:20 we're going to pick curve that is one-dimensional and it will hit every single points on earth when you cut the Earth up into a quad tree at some resolution right this is it this is a quad tree because for every bit of precision here we are going to cut the world into both in life
SLIDE53
01:17:40 also have four faces for every level right so this is called level 2 because it's got one subdivision and II subdivision in each Dimension so there's four total divisions in both X and Y and
01:18:00 face in this order it's fractal because within each of these phases if you go to the next level it will follow the same pattern so we call that we call that a fractal property and the quality of this curve is going to be measured by what we call locality locality means
01:18:20 in the space that we care about should be close to each other on the line as well and sorting by latitude longitude gives you like the worst locality that you can imagine because once you wrap around the world you know you're making a big jump this year
01:18:40 foxy jumping over a big part of space there are better curves to do a better job of them and I'll show you an example later but pretty good it's it's simple and easy to see and understand the reason we do this is because if we want to query a region of space
01:19:00 rain scanner for values along this curve right so in this case all of North America is you know encapsulated by you know two or three or four you no points here and we can do that in one single range so here's a level 2
01:19:20 there's level three lights it out as eight divisions so we get one more bit of precision in X and Y for every level that we we go up because it's fractal we can go if we go to level 32 that's a 64-bit integer and we can actually map every single Square centimeter on the earth to single integer along this
01:19:40 and it works it works pretty well here is a position along this line we can map any single point if we use the position in the high levels to be the most significant bits and the position of the lowest levels
01:20:00 then we get like a binary prefix straight so apparent cell will be like 0 1 0 1 1 1 and then all of its children sells we like real numbers between that with a bunch of zeros and that was a bunch of ones right so you can do a binary prefix to find the
SLIDE54
01:20:20 any given cell rates it's arranged skin I'm you can say you can turn those in Spanish using say everything between you know who sells 1 through 5 or whatever and those will be confined within a a specific geographical region so it ends up being quite sufficient for that
01:20:40 the way that we process this index weather this is spark or desk right imagine we have a dataframe and it's going to have a lot of food and longitude point for every row right cuz every road here's the points will then use whatever function were using geohash
01:21:00 particular function that gives us you know this this type of index and we can just kind of map that you know it's not the sort it by that says can require Shuffle but we sorted by Value store in parque or desk that gives us geospatial
SLIDE55
01:21:20 anything that we can do here is we can partition it by some level right to make sure that are you are partitions are compact so what I'm doing here is is given you know the Ensign sell at any given level is easy to find the smallest
01:21:40 the integer value for the the first cell in there and energy value flight the last selling their new can calculate those as integer boundaries for all of the points within that that reason right so we can say Hey you know that's just practicing this by every single high-level sell at some resolution of the world and we can be pretty
01:22:00 that doesn't require resorting once we've sorted my Shuffle little bit between the edges but the most Brothers just going to junk it up into you know by these faces right whether we're looking globally or
01:22:20 some small region the world we can decide a level that you don't gives us the number of partitions in with aspects of another day that we want rights pretty easy to do I actually continually promoted
SLIDE56
01:22:40 to a higher self level to producing them until they reach a certain data size so this is some actual data what I was looking at is the number of homes that I was going to evaluate a home score fork and I grew the partition until the number of homes was no greater than to the 13th
SLIDE57
01:23:00 alexandros / / partition that was what I decided and so these are the balance of every partition that I had when I did that so you noticed that like in the middle of the day I get a larger partition because I was able to grow it because their new homes
01:23:20 the way home Sears not actually houses it's it's literally like building geometries that I was able to query from openstreetmap so doesn't necessarily accurate with respect to CentraState of this is this is a particular dataset this is not easy order curve this is a Hilbert curve so it looks a little bit different
01:23:40 is you are the one that is serving the same purpose is just going to look kind of squiggly it makes these use and this is the best way to play Nibbler snakes if you ever want to have your wife yell at you for being way too nerdy so you can see that different protections are different sizes
01:24:00 and you noticed that like so this is the progression of that curve and so if I wanted to know. Like just downtown Boston you know it's pretty tightly confined within a shape and that I wanted I want to see it and this is the same plot just for
01:24:20 I'm so you noticed some you know very big reasons why we don't have a lot of Home data and so the participants get really big geographic regions are much smaller near downtown notice nicely that you know the Cape is basically all along this one bit of the curve because of locality Preserve
01:24:40 it's so it works like pretty well and again is fractal right there within each of these all the homes are still sorted right so we're looking at a high-level partitioning but I can still query like a small shape or arranged within each one of those partitions quite efficiently as well because
SLIDE58
01:25:00 at a lower level higher level so the way that this really manifest itself is that we don't actually want to query by the index right that's esoteric and
01:25:20 what is a shape so forgive me for jumping examples but this took aside from you know where the library is that Google released when it was doing the same thing for Google Maps one of the
SLIDE59
01:25:40 so they raised the question like what do you want a query for every tweet from Hawaii right so tweets have lots of Long's associate with them so we want to provide a shape to the query engine and what it's going to do is create what's called a covering so freely given shape we want to
01:26:00 set of cells along that index that cover the polygonal shape right and then we turn those cells into range scans because each of them is is just an integer in this space and we can find the smallest child cell and largest child cell of that
01:26:20 function that says hey take this polygon come up with a set of slices that represent the cells that explicitly cover this and that will give us our predicate pushdown and some form now we still have to go back and actually see whether any single row was was in the latitude longitude
01:26:40 bounding box of of that shape right do we still have to do A fine grain filter but this will do a very good job of quickly finding using that key value index using a very complex set of optimizations that are designed just for this purpose
01:27:00 we have you know 10 or so different cells right so that's 10 separate queries we can have fewer Aquarius that are larger or we can have more queries that are very very small cells that more explicitly cover right and so within a function like this you effectively you have a
01:27:20 has some tunable in a trade-off between the number of look up so you're going to do and the sort of false positive rate that your query will return right and so that's that's something that has to be optimized for but it can be done in this in this kind of context
01:27:40 whole thing is that allows you to do convolutions so what I really cared about was given a home which is a points find all of the other points in the neighborhood knows other points might be
01:28:00 so the home is the queer location and then I have a hold on the dataset that's also going to be indexed based on GPS my sentiment to say hey finally know the average acceleration of all of the new GPS signals in this area something like that so that's a very complex problems
SLIDE60
01:28:20 search for every other points and it becomes like an N squared problem right it's it's very very difficult but I can use index to sort of pre-filter that down so you can imagine some kind of neighborhood reduction problem right so you have some data frame that is indexed on the query
01:28:40 the neighborhood for every Row in that query dataframe and you have some other dataframe that you want to calculate a reduction of right and then you have whatever that function is going to calculate that and how you going to combine the results so you can imagine doing something like this
01:29:00 I'm going to start over here so I'm going to concatenate the results so take my query dataframe iterate through every single partition right I take the partition of that create dataframe and that
01:29:20 there's a single cell that covers it because of the way that we set these participants right so it's not going to have stuff on the other side of the world is going to be within some relatively confined geographic region and I'm passing in the entire other dataframe into this neighborhood Russian right so
01:29:40 based on the divisions of that sell we can figure out divisions of that date of random passing as a query we can figure out the covering sell for that and then we can simply find the neighbor cells and we can use those to slice into that
01:30:00 different because each one of them is is a device so I have my what are my function is that's going to you know maybe it's the same for every Point actually look for things that are actually closing up right going to do something and then it's going to whatever add up you know where average the
01:30:20 you know the acceleration in some areas or something like that right so you have to implement that however I want to implement it but this canal calculate that piecewise for every single slice of the other dataframe that has only you know the points in that region and then Adam
01:30:40 you know some something like that right so normally would have have to do this for every single partition in my initial data set times every single partition in the other data set and then how many rows are in each partition like this very complex process but we can use this slicing ability to drastically
01:31:00 competition save a lot of lot of effort lot of work
01:31:20 the point is that you know you can build stuff on top of this is indexing ability but with gastritis and I'm more than happy to talk people through this if anyone actually wants to do a project on geospatial stuff there's a lot of cool opportunities for that in desk they tried a few times to it to make it more
01:31:40 bring some of the some of the requests that I put in which I don't think I ever end up getting merge that kind of dropped the ball on that but is a couple of things out there if that's something that you're interested in other brief.
01:32:00 for the night is a computational engine and Luigi is a workflow graph and it's great to use them together and there's a few things that kind of get you no complicated.
01:32:20 we going to address in problem set form but I want to introduce you to some of the the complexity before we really get there right and so absolutely desperate complementary they're not going to compete against each other but there's a couple of things that you want to think about right so the first one is is the Target right so
01:32:40 BGR targets have been files and it's easy to write a file atomically porque cambia file but typically it's not in other other desk collections are collection of CSV to Json files
01:33:00 if you're on in a local disk you can still create a temporary folder write your collection out to that and then move it to your destination and everything will be Atomic that's kind of okay but that's not really how big data
01:33:20 especially if you're can't move things on the S3 right there that's where it is you have to copy it and redownload it been traditionally like you might have a lot of parts right and I might take a long time to do so so the way we've been thinking about Atomic targets
01:33:40 doesn't really translate well into big data or data Collections and we have to rethink that a little bit we still want that exists functionality we need to know whether the target has been finished but we're not going to write them atomically in the same way
01:34:00 look for the existence of a folder or a file one thing that's in the Luigi Library already is an S3 flag Target that was thrown in in a CFS flag Target and that's turning because typically when you're dealing with something like Spa
01:34:20 you know that a folder is ready to be consumed is a dataset is because it writes this success file so it's some folder / than they write underscore success when the task successfully finishes that's not something that does but it's something that we can easily add in to it to make desk work well
SLIDE61
01:34:40 Target so doesn't give us atomicity of the reading right but it does give us the ability to atomically say that this data set has been finished correctly so this is not a good implementation of this but it will give you the idea right this is only going to work for a local
01:35:00 be dealing with iOS top half. Joint and basically we have some path for that Target right into the exists method for the desk Luigi Target is going to look for this success file or you can parametrized that if you wanted but it's going to be looking for this file underneath you know the directory
01:35:20 what that means is that we want to use the target to write the dust collection right so instead of saying some dataframe. 2 parque we might have to have a parquet desk Target and we want to say okay. Target. Right I'm passing the different because it's going to need to do
01:35:40 after it's written the thing it goes in and creates that success file so it might have it look something like this right collection. 2 parque because this thing calls you know self. Right and then you can imagine having glasses for different types of.
01:36:00 I want to have a read function on a 2 just so they have some interface right we can build a desk Target interface which has a read and write something like that what's wrong with this implementation
01:36:20 it is this will work for
01:36:40 your file system but desk and Luigi have different ideas of what a file system is so Luigi has like a file system Target which is the base class for your S3 file Target that you've been playing with as well as the local Target Louis
01:37:00 interface and it is deals with atomic writing to and from. But basketball miss its own file system interface which is not entirely consistent but it can write two things like S3 or hdfs or now azure ffs
01:37:20 Apple desk and you want to make sure that you are not trying to use what you learned in the Ouija to read and write files when it's task that's doing it and it's going to be no just require a different different package so obviously this is
01:37:40 so we need to hook into the doc file system targets soothing the desk file system protocol in a different way than we did for Luigi and that's a little bit complicated so I didn't want to walk you through that logic and slide form we're either going
01:38:00 right part of that as a problem set or I'm going to give you some of the code cuz it's a little bit a little bit complex but that's that's the way that the solution looks is that you need to hook into the desk file system protocol and tell it to use the same protocol with the same credentials that you you know you say so
01:38:20 do you like extra whatever needs to be configured with a desk needs to be configured an enemy to leverage that to add this success flag as well as a couple of other but it does help to have this target even if you don't use Luigi
01:38:40 doc datasets into targets like this is really really beneficial for indicating that it has been written completely and also somewhat standardizing the interface tries to say Target. Rejected our right as opposed to doing with those different collections
01:39:00 is not correct code don't try to use it directly you could use this temporary but that only works locally exists saying exist on a directory itself isn't really good right if you even if you just called us. Right to send record
01:39:20 advise the file so we still want to flag file rather than checking at the directory itself exists and there's no given file that we can trust is going to be there all of the time other than that successful I avoid OS
SLIDE62
01:39:40 written Ford asked to use a series of file system isofresh 3 we have to use that and there's kind of interface not actually the same interface for all of them witches annoying to a micro there's a couple of ugly duckling points that they use but will get you through all of that in order to use these things in harmony
01:40:00 some of these I think I've shown you before but if you haven't made it through all of the sort of columnar stuff there in a couple of newspapers and in write-ups that I found a little bit more about this split apply combine and
01:40:20 of desk in section we've been going through some desk Labs that went out of Pikkon and I think that those are our pretty cool I when I get through all of them but I think we're probably sharing the resources for that is pretty cool
SLIDE63
01:40:40 hopefully it'll be reflected in your jobs going forward all right that is content for the night
01:41:00 how to use AR-15 extended days for what's the process to do that
01:41:20 so if you can so cuz it's due tonight it'll be nice if you just go in and you add a comments on the grade on this mission in canvas you can just say I submit it
01:41:40 before you can comment is that right away to turn it off
01:42:00 master I waited till until I was ready to submit it to spit and then put a comment in there because you couldn't you couldn't submit a comment without the actual like
SLIDE64
